{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_json, use_tf_idf_vectorizer, use_count_vectorizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.read_csv(\"train_pred.csv\")\n",
    "train_pred.rename(\n",
    "    {\n",
    "        \"winner_model_a\": \"winner_model_a_pred_prob\", \n",
    "        \"winner_model_b\": \"winner_model_b_pred_prob\", \n",
    "        \"winner_tie\": \"winner_tie_pred_prob\"\n",
    "    }, \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")\n",
    "print(train_pred.head())\n",
    "print(train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(train_pred, on=\"id\", how=\"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"model_a\", \"model_b\"], inplace=True)\n",
    "df[\"label\"] = df.apply(lambda row: 0 if row[\"winner_model_a\"] else (1 if row[\"winner_model_b\"] else 2), axis=1)\n",
    "df.drop(columns=[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_json(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_length_sum(response):\n",
    "    length_sum = sum(len(s) if s is not None else 0 for s in response)\n",
    "    return length_sum\n",
    "\n",
    "def calculate_total_words(response):\n",
    "    total_words = sum(len(s.split()) if s is not None else 0 for s in response)\n",
    "    return total_words\n",
    "\n",
    "df[\"len_a\"] = df[\"response_a\"].apply(calculate_length_sum)\n",
    "df[\"len_b\"] = df[\"response_b\"].apply(calculate_length_sum)\n",
    "df[\"word_a\"] = df[\"response_a\"].apply(calculate_total_words)\n",
    "df[\"word_b\"] = df[\"response_b\"].apply(calculate_total_words)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_strings(string_list):\n",
    "    filtered_list = [s if s else \"NULL\" for s in string_list]\n",
    "    return \" \".join(filtered_list)\n",
    "\n",
    "df[\"prompt\"] = df[\"prompt\"].apply(join_strings)\n",
    "df[\"response_a\"] = df[\"response_a\"].apply(join_strings)\n",
    "df[\"response_b\"] = df[\"response_b\"].apply(join_strings)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tf_idf_vectorizer, prompt_tf_idf_matrix = use_tf_idf_vectorizer(train=df, column_name=\"prompt\")\n",
    "response_a_tf_idf_vectorizer, response_a_tf_idf_matrix = use_tf_idf_vectorizer(train=df, column_name=\"response_a\")\n",
    "response_b_tf_idf_vectorizer, response_b_tf_idf_matrix = use_tf_idf_vectorizer(train=df, column_name=\"response_b\")\n",
    "print(\"The shape of prompt_tf_idf_matrix is:\", prompt_tf_idf_matrix.shape)\n",
    "print(\"The shape of response_a_tf_idf_matrix is:\", response_a_tf_idf_matrix.shape)\n",
    "print(\"The shape of response_b_tf_idf_matrix is:\", response_b_tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_count_vectorizer, prompt_count_matrix = use_count_vectorizer(train=df, column_name=\"prompt\")\n",
    "# response_a_count_vectorizer, response_a_count_matrix = use_count_vectorizer(train=df, column_name=\"response_a\")\n",
    "# response_b_count_vectorizer, response_b_count_matrix = use_count_vectorizer(train=df, column_name=\"response_b\")\n",
    "# print(\"The shape of prompt_count_matrix is:\", prompt_count_matrix.shape)\n",
    "# print(\"The shape of response_a_count_matrix is:\", response_a_count_matrix.shape)\n",
    "# print(\"The shape of response_b_count_matrix is:\", response_b_count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(prompt_tf_idf_matrix, on=\"id\", how=\"left\")\n",
    "df = df.merge(response_a_tf_idf_matrix, on=\"id\", how=\"left\")\n",
    "df = df.merge(response_b_tf_idf_matrix, on=\"id\", how=\"left\")\n",
    "print(\"The shape of df after merging all tf_idf matrix is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.merge(prompt_count_matrix, on=\"id\", how=\"left\")\n",
    "# df = df.merge(response_a_count_matrix, on=\"id\", how=\"left\")\n",
    "# df = df.merge(response_b_count_matrix, on=\"id\", how=\"left\")\n",
    "# print(\"The shape of df after merging all count matrix is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lgbm_models\"):\n",
    "    os.makedirs(\"lgbm_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"len_a\"] = df[\"len_a\"].astype(float)\n",
    "df[\"len_b\"] = df[\"len_b\"].astype(float)\n",
    "df[\"word_a\"] = df[\"word_a\"].astype(float)\n",
    "df[\"word_b\"] = df[\"word_b\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"id\", \"prompt\", \"response_a\", \"response_b\", \"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "columns_to_normalize = [\"len_a\", \"len_b\", \"word_a\", \"word_b\"]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "log_loss_scores = []\n",
    "\n",
    "for idx, (train_index, val_index) in tqdm(enumerate(kf.split(X)), total=n_splits):\n",
    "    print(f\"---------- Fold {idx + 1} ----------\")\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    \n",
    "    X_train.loc[:, columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "    X_val.loc[:, columns_to_normalize] = scaler.transform(X_val[columns_to_normalize])\n",
    "    \n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": 3,\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"n_estimators\": 2048,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"colsample_bytree\": 0.75,\n",
    "        \"num_leaves\": 18,\n",
    "        \"max_depth\": 12,\n",
    "        \"verbose\": 1\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    early_stopping_callback = lgb.early_stopping(50, first_metric_only=True, verbose=True)\n",
    "\n",
    "    model.fit(\n",
    "        X=X_train, y=y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_proba(X_val, num_iteration=model.best_iteration_)\n",
    "    # print(y_pred)\n",
    "\n",
    "    model.booster_.save_model(f\"./lgbm_models/lgbm_model_fold_{idx + 1}.txt\")\n",
    "\n",
    "    # To load the model\n",
    "    # model = lgb.Booster(model_file=f\"./lgbm_models/lgbm_model_fold_{idx + 1}.txt\")\n",
    "    \n",
    "    score = log_loss(y_val, y_pred)\n",
    "    log_loss_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {idx + 1} log_loss: {score}\")\n",
    "\n",
    "mean_log_loss = np.mean(log_loss_scores)\n",
    "print(f\"Mean log_loss: {mean_log_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
