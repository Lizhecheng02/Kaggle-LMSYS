train_file_path: "../data/train.csv"
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
learning_rate: 2.0e-4
warmup_ratio: 0.1
num_train_epochs: 3
gradient_accumulation_steps: 16
logging_steps: 100
eval_steps: 200
save_steps: 200
weight_decay: 1.e-3
save_total_limit: 10
max_length: 3000
MODEL: "google/gemma-2b"
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05