train_file_path: "../data/train.csv"
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
learning_rate: 2.0e-4
warmup_ratio: 0.1
num_cycles: 1.5
num_train_epochs: 3
gradient_accumulation_steps: 16
logging_steps: 200
eval_steps: 200
save_steps: 200
weight_decay: 1.0e-3
save_total_limit: 5
max_length: 4096
model_name: "google/gemma-2b"
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
huggingface_api: "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"