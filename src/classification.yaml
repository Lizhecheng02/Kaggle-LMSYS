train_file_path: "../data/train.csv"
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
learning_rate: 2.0e-4
warmup_ratio: 0.1
num_train_epochs: 3
gradient_accumulation_steps: 16
logging_steps: 10
eval_steps: 10
save_steps: 10
weight_decay: 1.e-3
save_total_limit: 10
max_length: 1024
model_name: "google/gemma-2b"
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
huggingface_api: "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"