{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727873d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:24:58.633099Z",
     "iopub.status.busy": "2024-08-02T18:24:58.632803Z",
     "iopub.status.idle": "2024-08-02T18:25:39.440010Z",
     "shell.execute_reply": "2024-08-02T18:25:39.438728Z"
    },
    "papermill": {
     "duration": 40.823965,
     "end_time": "2024-08-02T18:25:39.442743",
     "exception": false,
     "start_time": "2024-08-02T18:24:58.618778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files\n",
    "!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/lmsys-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6926c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:25:39.472524Z",
     "iopub.status.busy": "2024-08-02T18:25:39.471953Z",
     "iopub.status.idle": "2024-08-02T18:25:47.948464Z",
     "shell.execute_reply": "2024-08-02T18:25:47.947562Z"
    },
    "papermill": {
     "duration": 8.493814,
     "end_time": "2024-08-02T18:25:47.950801",
     "exception": false,
     "start_time": "2024-08-02T18:25:39.456987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps --no-index /kaggle/input/hf-libraries/transformers/transformers-4.43.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33736c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:25:47.982385Z",
     "iopub.status.busy": "2024-08-02T18:25:47.981993Z",
     "iopub.status.idle": "2024-08-02T18:25:48.003078Z",
     "shell.execute_reply": "2024-08-02T18:25:48.002144Z"
    },
    "papermill": {
     "duration": 0.039556,
     "end_time": "2024-08-02T18:25:48.004889",
     "exception": false,
     "start_time": "2024-08-02T18:25:47.965333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile gemma_inference.py\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import  GemmaTokenizerFast, BitsAndBytesConfig, Gemma2ForCausalLM\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel\n",
    "\n",
    "assert torch.cuda.device_count() == 2\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/googlegemma-2-9b-it'\n",
    "    lora_dir = '/kaggle/input/gemma2-len-2200-all-train/checkpoint-5750'\n",
    "    max_length = 1900\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Load & pre-process Data \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "if len(test)<= 10:\n",
    "    test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv', nrows=100)\n",
    "\n",
    "def process(input_str):\n",
    "    return json.loads(input_str)\n",
    "original_length = len(test)\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "test = test.explode(['prompt','response_a','response_b']).reset_index(drop=True)\n",
    "test = test.fillna('None')\n",
    "test['response_a'] = test['response_a'].apply(lambda x: 'None' if len(x) == 0 else x)\n",
    "test['response_b'] = test['response_b'].apply(lambda x: 'None' if len(x) == 0 else x)\n",
    "\n",
    "def get_text_length(text):\n",
    "    '''\n",
    "    不用空格分隔的文本, text length = len\n",
    "    不用空格分隔的一般tokenizer后长度类似，所以还可以缩小\n",
    "    空格分隔的，len(text.split(\" \"))\n",
    "    '''\n",
    "    length1 = len(text)\n",
    "    length2 = len(text.split(\" \"))\n",
    "    #远超过\n",
    "    if length1 >= length2 * 30 and length1>= 300:\n",
    "        return length1 * 0.75\n",
    "    return length2\n",
    "    \n",
    "def prompt_3(data, max_length, if_train):\n",
    "    '''\n",
    "    超过max length新开一行，label不变\n",
    "    从后往前拼接\n",
    "    #Prompt1\n",
    "    xxxx\n",
    "    #Response\n",
    "    ##Model A\n",
    "    xxxx\n",
    "    ##Model B\n",
    "    xxxx\n",
    "    \n",
    "    #Prompt2\n",
    "    #Response\n",
    "    ##Model A\n",
    "    xxxx\n",
    "    ##Model B\n",
    "    xxxx\n",
    "    '''\n",
    "\n",
    "    data['prompt_response'] = \"#Prompt\\n\" + data['prompt'] + \"\\n\\n\" + \"#Response\\n\" + \"##Model A\\n\" + data['response_a'] + \"\\n\\n\" + \"##Model B\\n\" + data['response_b']\n",
    "    data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    prompt_response = []\n",
    "    ids = []\n",
    "    labels = []\n",
    "    #只有一种可能会超出max length：\n",
    "    #单条的prompt和reponse加在一起超出max length\n",
    "    over_max_length = [] #是否有超出max length的部分\n",
    "    overflow_prompt = []\n",
    "    overflow_response_a = [] #超出max length的部分\n",
    "    overflow_response_b = [] #超出max length的部分\n",
    "    text_length = 0\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        text = row['prompt_response']\n",
    "        response_a = row['response_a']\n",
    "        response_b = row['response_b']\n",
    "        prompt = row['prompt']\n",
    "        id = row['id']\n",
    "        \n",
    "        if if_train:\n",
    "            label = row['label']\n",
    "        \n",
    "        if id not in ids:\n",
    "            #第一次出现\n",
    "            prompt_response.append(text)\n",
    "            text_length = get_text_length(text)\n",
    "            ids.append(id)\n",
    "            if if_train:\n",
    "                labels.append(label)\n",
    "            if text_length > max_length:\n",
    "                over_max_length.append(1)\n",
    "                overflow_prompt.append(prompt)\n",
    "                overflow_response_a.append(response_a)\n",
    "                overflow_response_b.append(response_b)\n",
    "            else:\n",
    "                over_max_length.append(0)\n",
    "                overflow_prompt.append(None)\n",
    "                overflow_response_a.append(None)\n",
    "                overflow_response_b.append(None)\n",
    "        \n",
    "        else:\n",
    "            text_length += get_text_length(text)\n",
    "            if text_length <= max_length:\n",
    "                #取上一个text出来，合并后替换\n",
    "                text = text + \"\\n\\n\" + prompt_response[-1]\n",
    "                prompt_response[-1] = text\n",
    "                over_max_length[-1] = 0\n",
    "                overflow_prompt[-1] = None\n",
    "                overflow_response_a[-1] = None\n",
    "                overflow_response_b[-1] = None\n",
    "                \n",
    "            else:\n",
    "                #另一起一行\n",
    "                prompt_response.append(text)\n",
    "                text_length = get_text_length(text)\n",
    "                ids.append(id)\n",
    "                \n",
    "                if if_train:\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                #另起一行但超出场合都\n",
    "                if text_length > max_length:\n",
    "                    over_max_length.append(1)\n",
    "                    overflow_prompt.append(prompt)\n",
    "                    overflow_response_a.append(response_a)\n",
    "                    overflow_response_b.append(response_b)\n",
    "                else:\n",
    "                    over_max_length.append(0)\n",
    "                    overflow_prompt.append(None)\n",
    "                    overflow_response_a.append(None)\n",
    "                    overflow_response_b.append(None)\n",
    "                    \n",
    "                \n",
    "                    \n",
    "    if if_train:           \n",
    "        data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, \"label\": labels, 'overflow_prompt': overflow_prompt, 'over_max_length': over_max_length, 'overflow_response_a': overflow_response_a, 'overflow_response_b': overflow_response_b})\n",
    "        data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    else:\n",
    "        data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, 'over_max_length': over_max_length, 'overflow_prompt': overflow_prompt, 'overflow_response_a': overflow_response_a, 'overflow_response_b': overflow_response_b})\n",
    "        data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    return data\n",
    "\n",
    "test = prompt_3(test, cfg.max_length * 0.75, False)\n",
    "test = test.drop_duplicates(subset=['id'], keep='last').reset_index(drop=True)\n",
    "assert len(test) == original_length\n",
    "\n",
    "# tokenize\n",
    "\n",
    "def tokenize(tokenizer, data):\n",
    "    prompts = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        now_data = data.loc[i]\n",
    "        idx = now_data['id']\n",
    "        \n",
    "        over_max_length = now_data['over_max_length']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "\n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "\n",
    "        templete_part4_input_ids = tokenizer(text=\"\\n\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "\n",
    "        if over_max_length:\n",
    "            prompt = \"#Prompt\\n\" + now_data['overflow_prompt']\n",
    "            r_a = \"#Response\\n\" + \"##Model A\\n\" + now_data['overflow_response_a']\n",
    "            r_b = \"##Model B\\n\" + now_data['overflow_response_b']\n",
    "\n",
    "            prompt_ids = tokenizer(text=prompt, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_a_input_ids = tokenizer(text=r_a, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_b_input_ids = tokenizer(text=r_b, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "\n",
    "            if len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids) <= cfg.max_length:\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "\n",
    "            else:\n",
    "                '''\n",
    "                prompt 和 response 按照 300， 800， 800\n",
    "                response 优先\n",
    "                多的再给prompt\n",
    "                '''\n",
    "                length = [len(prompt_ids), len(model_a_input_ids), len(model_b_input_ids)]\n",
    "                print(f\"before {len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids)}\")\n",
    "                print(f\"before {length}\")\n",
    "                prompt_max_length, a_max_length, b_max_length = adjust(length)\n",
    "\n",
    "                prompt_ids = prompt_ids[:prompt_max_length] + templete_part4_input_ids\n",
    "                model_a_input_ids = model_a_input_ids[:a_max_length] + templete_part4_input_ids\n",
    "                model_b_input_ids = model_b_input_ids[:b_max_length] + templete_part4_input_ids\n",
    "\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "                print(f\"after {[prompt_max_length, a_max_length, b_max_length]}\")\n",
    "                print(f\"after {len(prompt_response_ids)}\")\n",
    "\n",
    "        else:\n",
    "            prompt_response = now_data['prompt_response']\n",
    "            prompt_response_ids = tokenizer(text=prompt_response, add_special_tokens=True, truncation=True, max_length=cfg.max_length, padding=False)['input_ids'][1:]    \n",
    "\n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = tokenizer.decode(input_ids[1:], skip_special_tokens=False)\n",
    "        if i == 0:\n",
    "            print(input_text)\n",
    "        prompts.append(input_text)\n",
    "    tokenized = tokenizer(prompts)\n",
    "    input_ids = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask \n",
    "\n",
    "def adjust_values(A, B, a_space, b_space, ex_space):\n",
    "    # 计算A和a_space的差值\n",
    "    a_diff = a_space - A\n",
    "    b_diff = b_space - B\n",
    "    \n",
    "    # 第一种情况：A小于a_space，B小于b_space\n",
    "    if A < a_space and B < b_space:\n",
    "        ex_space += a_diff + b_diff\n",
    "        return A, B, ex_space\n",
    "\n",
    "    # 第二种情况：如果A和B都各自大于自己的space\n",
    "    elif A > a_space and B > b_space:\n",
    "        total_extra_needed = (A - a_space) + (B - b_space)\n",
    "        if total_extra_needed > ex_space:\n",
    "            A = int(a_space + ex_space / 2)\n",
    "            B = int(b_space + ex_space / 2)\n",
    "            ex_space = 0\n",
    "        else:\n",
    "            a_space = A\n",
    "            b_space = B\n",
    "            ex_space -= total_extra_needed\n",
    "            \n",
    "        return A, B, ex_space\n",
    "        \n",
    "    # 第三种情况：A或者B其中有一个大于a_space, b_space\n",
    "    elif A >= a_space or B >= b_space:\n",
    "        # 如果A大于a_space但是B小于b_space\n",
    "        if A >= a_space and B <= b_space:\n",
    "            extra_needed = A - a_space\n",
    "            ex_space += b_space - B\n",
    "            #够用\n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                #不够用\n",
    "                #b_space = B + available_space\n",
    "                A = a_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        # 如果B大于b_space但是A小于a_space\n",
    "        elif B > b_space and A < a_space:\n",
    "            extra_needed = B - b_space\n",
    "            ex_space += a_space - A\n",
    "            \n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                B = b_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        return A, B, ex_space\n",
    "    \n",
    "\n",
    "def adjust(current_lengths, prompt_length_space=300, response_length_space=800):\n",
    "    prompt_length = current_lengths[0]\n",
    "    response_a_length = current_lengths[1]\n",
    "    response_b_length = current_lengths[2]\n",
    "    #先看prompt的额度\n",
    "    ex_space = max(0, prompt_length_space - prompt_length)\n",
    "    response_a_length, response_b_length, ex_space = adjust_values(response_a_length, response_b_length, response_length_space, response_length_space, ex_space)\n",
    "    prompt_length = min(prompt_length, prompt_length_space)\n",
    "    prompt_length += ex_space\n",
    "\n",
    "    return prompt_length, response_a_length, response_b_length\n",
    "\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "# tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test)\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))\n",
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "class CustomGemma2ForCausalLM(Gemma2ForCausalLM):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "        if self.training and self.config._attn_implementation != \"eager\":\n",
    "            logger.warning_once(\n",
    "                \"It is strongly recommended to train Gemma2 models with the `eager` attention implementation \"\n",
    "                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n",
    "            )\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0][:,-1]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if self.config.final_logit_softcapping is not None:\n",
    "            logits = logits / self.config.final_logit_softcapping\n",
    "            logits = torch.tanh(logits)\n",
    "            logits = logits * self.config.final_logit_softcapping\n",
    "\n",
    "        logits = logits.float()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "# Load base model on GPU 0\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = CustomGemma2ForCausalLM.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = CustomGemma2ForCausalLM.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n",
    "\n",
    "model_0.eval()\n",
    "model_1.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference_test(df, model, device, batch_size=cfg.batch_size):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        outputs = model(**inputs.to(device))\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = ((outputs.logits.cpu()[:,A_TOKEN_IDS + B_TOKEN_IDS + C_TOKEN_IDS]) / 1.03).softmax(-1)\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission_gemma.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e0bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:25:48.034877Z",
     "iopub.status.busy": "2024-08-02T18:25:48.034623Z",
     "iopub.status.idle": "2024-08-02T18:25:48.053542Z",
     "shell.execute_reply": "2024-08-02T18:25:48.052674Z"
    },
    "papermill": {
     "duration": 0.036938,
     "end_time": "2024-08-02T18:25:48.055812",
     "exception": false,
     "start_time": "2024-08-02T18:25:48.018874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile llama_inference.py\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import  GemmaTokenizerFast, BitsAndBytesConfig, Gemma2ForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/llama31instruct/Meta-Llama-3.1-8B-Instruct'\n",
    "    lora_dir = '/kaggle/input/llama3-1-all-data/checkpoint-5768'\n",
    "    max_length = 2400\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "def get_data(path='/kaggle/input/lmsys-chatbot-arena/test.csv', reverse=False):\n",
    "    \n",
    "    test = pd.read_csv(path)\n",
    "    \n",
    "    if len(test)<= 10:\n",
    "        test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv', nrows=100)\n",
    "\n",
    "        \n",
    "    if reverse:\n",
    "        test['response_a'], test['response_b'] = test['response_b'], test['response_a']\n",
    "    \n",
    "    def process(input_str):\n",
    "        return json.loads(input_str)\n",
    "    \n",
    "    original_length = len(test)\n",
    "    test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "    test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "    test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "    test = test.explode(['prompt','response_a','response_b']).reset_index(drop=True)\n",
    "    test = test.fillna('None')\n",
    "    test['response_a'] = test['response_a'].apply(lambda x: 'None' if len(x) == 0 else x)\n",
    "    test['response_b'] = test['response_b'].apply(lambda x: 'None' if len(x) == 0 else x)\n",
    " \n",
    "    def prompt_3(data, max_length, if_train):\n",
    "        '''\n",
    "        超过max length新开一行，label不变\n",
    "        从后往前拼接\n",
    "        #Prompt1\n",
    "        xxxx\n",
    "        #Response\n",
    "        ##Model A\n",
    "        xxxx\n",
    "        ##Model B\n",
    "        xxxx\n",
    "\n",
    "        #Prompt2\n",
    "        #Response\n",
    "        ##Model A\n",
    "        xxxx\n",
    "        ##Model B\n",
    "        xxxx\n",
    "        '''\n",
    "\n",
    "        data['prompt_response'] = \"#Prompt\\n\" + data['prompt'] + \"\\n\\n\" + \"#Response\\n\" + \"##Model A\\n\" + data['response_a'] + \"\\n\\n\" + \"##Model B\\n\" + data['response_b']\n",
    "        data = data.iloc[::-1].reset_index(drop=True)#反转\n",
    "        prompt_response = []\n",
    "        ids = []\n",
    "        labels = []\n",
    "        #只有一种可能会超出max length：\n",
    "        #单条的prompt和reponse加在一起超出max length\n",
    "        over_max_length = [] #是否有超出max length的部分\n",
    "        overflow_prompt = []\n",
    "        overflow_response_a = [] #超出max length的部分\n",
    "        overflow_response_b = [] #超出max length的部分\n",
    "        text_length = 0\n",
    "        for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "            text = row['prompt_response']\n",
    "            response_a = row['response_a']\n",
    "            response_b = row['response_b']\n",
    "            prompt = row['prompt']\n",
    "            id = row['id']\n",
    "\n",
    "            if if_train:\n",
    "                label = row['label']\n",
    "\n",
    "            if id not in ids:\n",
    "                #第一次出现\n",
    "                prompt_response.append(text)\n",
    "                text_length = len(text.split(\" \"))\n",
    "                ids.append(id)\n",
    "                if if_train:\n",
    "                    labels.append(label)\n",
    "                if text_length > max_length:\n",
    "                    over_max_length.append(1)\n",
    "                    overflow_prompt.append(prompt)\n",
    "                    overflow_response_a.append(response_a)\n",
    "                    overflow_response_b.append(response_b)\n",
    "                else:\n",
    "                    over_max_length.append(0)\n",
    "                    overflow_prompt.append(None)\n",
    "                    overflow_response_a.append(None)\n",
    "                    overflow_response_b.append(None)\n",
    "\n",
    "            else:\n",
    "                text_length += len(text.split(\" \"))\n",
    "                if text_length <= max_length:\n",
    "                    #取上一个text出来，合并后替换\n",
    "                    text = text + \"\\n\\n\" + prompt_response[-1]\n",
    "                    prompt_response[-1] = text\n",
    "                    over_max_length[-1] = 0\n",
    "                    overflow_prompt[-1] = None\n",
    "                    overflow_response_a[-1] = None\n",
    "                    overflow_response_b[-1] = None\n",
    "\n",
    "                else:\n",
    "                    #另一起一行\n",
    "                    prompt_response.append(text)\n",
    "                    text_length = len(text.split(\" \"))\n",
    "                    ids.append(id)\n",
    "\n",
    "                    if if_train:\n",
    "                        labels.append(label)\n",
    "\n",
    "                    #另起一行但超出场合都\n",
    "                    if text_length > max_length:\n",
    "                        over_max_length.append(1)\n",
    "                        overflow_prompt.append(prompt)\n",
    "                        overflow_response_a.append(response_a)\n",
    "                        overflow_response_b.append(response_b)\n",
    "                    else:\n",
    "                        over_max_length.append(0)\n",
    "                        overflow_prompt.append(None)\n",
    "                        overflow_response_a.append(None)\n",
    "                        overflow_response_b.append(None)\n",
    "\n",
    "\n",
    "\n",
    "        if if_train:           \n",
    "            data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, \"label\": labels, 'overflow_prompt': overflow_prompt, 'over_max_length': over_max_length, 'overflow_response_a': overflow_response_a, 'overflow_response_b': overflow_response_b})\n",
    "            data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "        else:\n",
    "            data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, 'over_max_length': over_max_length, 'overflow_prompt': overflow_prompt, 'overflow_response_a': overflow_response_a, 'overflow_response_b': overflow_response_b})\n",
    "            data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "        return data\n",
    "    test = prompt_3(test, cfg.max_length * 0.75, False)\n",
    "    test = test.drop_duplicates(subset = ['id'], keep ='last').reset_index(drop = True)\n",
    "    assert len(test) == original_length\n",
    "    return test\n",
    "\n",
    "\n",
    "# test = get_data()\n",
    "test = get_data(reverse = True)\n",
    "\n",
    "def tokenize(tokenizer, data):\n",
    "    prompts = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        now_data = data.loc[i]\n",
    "        idx = now_data['id']\n",
    "        \n",
    "        over_max_length = now_data['over_max_length']\n",
    "        templete_part1 = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "\n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        templete_part2_input_ids = tokenizer(text=templete_part2, add_special_tokens=False, padding=False)['input_ids']\n",
    "\n",
    "        templete_part4_input_ids = tokenizer(text=\"\\n\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "\n",
    "        if over_max_length:\n",
    "            prompt = \"#Prompt\\n\" + now_data['overflow_prompt']\n",
    "            r_a = \"#Response\\n\" + \"##Model A\\n\" + now_data['overflow_response_a']\n",
    "            r_b = \"##Model B\\n\" + now_data['overflow_response_b']\n",
    "\n",
    "            prompt_ids = tokenizer(text=prompt, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_a_input_ids = tokenizer(text=r_a, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_b_input_ids = tokenizer(text=r_b, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "\n",
    "            if len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids) <= cfg.max_length:\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "\n",
    "            else:\n",
    "                '''\n",
    "                prompt 和 response 按照 300， 800， 800\n",
    "                response 优先\n",
    "                多的再给prompt\n",
    "                '''\n",
    "                length = [len(prompt_ids), len(model_a_input_ids), len(model_b_input_ids)]\n",
    "                prompt_max_length, a_max_length, b_max_length = adjust(length)\n",
    "\n",
    "                prompt_ids = prompt_ids[:prompt_max_length] + templete_part4_input_ids\n",
    "                model_a_input_ids = model_a_input_ids[:a_max_length] + templete_part4_input_ids\n",
    "                model_b_input_ids = model_b_input_ids[:b_max_length] + templete_part4_input_ids\n",
    "\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "\n",
    "        else:\n",
    "            prompt_response = now_data['prompt_response']\n",
    "            prompt_response_ids = tokenizer(text=prompt_response, add_special_tokens=False, truncation=True, max_length=cfg.max_length, padding=False)['input_ids']   \n",
    "\n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids\n",
    "        input_text = tokenizer.decode(input_ids[:], skip_special_tokens=False)\n",
    "        if i == 0:\n",
    "            print(input_text)\n",
    "        prompts.append(input_text)\n",
    "    tokenized = tokenizer(prompts)\n",
    "    input_ids = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask \n",
    "\n",
    "def adjust_values(A, B, a_space, b_space, ex_space):\n",
    "    # 计算A和a_space的差值\n",
    "    a_diff = a_space - A\n",
    "    b_diff = b_space - B\n",
    "    \n",
    "    # 第一种情况：A小于a_space，B小于b_space\n",
    "    if A < a_space and B < b_space:\n",
    "        ex_space += a_diff + b_diff\n",
    "        return A, B, ex_space\n",
    "\n",
    "    # 第二种情况：如果A和B都各自大于自己的space\n",
    "    elif A > a_space and B > b_space:\n",
    "        total_extra_needed = (A - a_space) + (B - b_space)\n",
    "        if total_extra_needed > ex_space:\n",
    "            A = int(a_space + ex_space / 2)\n",
    "            B = int(b_space + ex_space / 2)\n",
    "            ex_space = 0\n",
    "        else:\n",
    "            a_space = A\n",
    "            b_space = B\n",
    "            ex_space -= total_extra_needed\n",
    "            \n",
    "        return A, B, ex_space\n",
    "        \n",
    "    # 第三种情况：A或者B其中有一个大于a_space, b_space\n",
    "    elif A >= a_space or B >= b_space:\n",
    "        # 如果A大于a_space但是B小于b_space\n",
    "        if A >= a_space and B <= b_space:\n",
    "            extra_needed = A - a_space\n",
    "            ex_space += b_space - B\n",
    "            #够用\n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                #不够用\n",
    "                #b_space = B + available_space\n",
    "                A = a_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        # 如果B大于b_space但是A小于a_space\n",
    "        elif B > b_space and A < a_space:\n",
    "            extra_needed = B - b_space\n",
    "            ex_space += a_space - A\n",
    "            \n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                B = b_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        return A, B, ex_space\n",
    "    \n",
    "\n",
    "def adjust(current_lengths, prompt_length_space=300, response_length_space=800):\n",
    "    prompt_length = current_lengths[0]\n",
    "    response_a_length = current_lengths[1]\n",
    "    response_b_length = current_lengths[2]\n",
    "    #先看prompt的额度\n",
    "    ex_space = max(0, prompt_length_space - prompt_length)\n",
    "    response_a_length, response_b_length, ex_space = adjust_values(response_a_length, response_b_length, response_length_space, response_length_space, ex_space)\n",
    "    prompt_length = min(prompt_length, prompt_length_space)\n",
    "    prompt_length += ex_space\n",
    "\n",
    "    return prompt_length, response_a_length, response_b_length\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.gemma_dir, trust_remote_code=True, truncation_side='left')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test)\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)\n",
    "\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))\n",
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "from transformers import LlamaForCausalLM\n",
    "class CustomLlamaForCausalLM(LlamaForCausalLM):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "        \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs[0][:,-1]\n",
    "#         hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "# Load base model on GPU 0\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = CustomLlamaForCausalLM.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "print(tokenizer.pad_token_id)\n",
    "model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "model_0.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load base model on GPU 1\n",
    "\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = CustomLlamaForCausalLM.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "#model_1.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n",
    "\n",
    "model_0.eval()\n",
    "model_1.eval()\n",
    "\n",
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=False, truncation=True, max_length=1024)['input_ids']\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=False, truncation=True, max_length=1024)['input_ids']\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=False, truncation=True, max_length=1024)['input_ids']\n",
    "print(A_TOKEN_IDS, B_TOKEN_IDS, C_TOKEN_IDS)\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "            \n",
    "        outputs = model(**inputs)\n",
    "        # proba = ((outputs.logits.cpu()[:, A_TOKEN_IDS + B_TOKEN_IDS + C_TOKEN_IDS])).softmax(-1)\n",
    "        proba = ((outputs.logits.cpu()[:, A_TOKEN_IDS + B_TOKEN_IDS + C_TOKEN_IDS]) / 1.03).softmax(-1)\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "tta_result_df = pd.concat(list(results), axis=0)\n",
    "tta_result_df = tta_result_df.sort_values('id')\n",
    "\n",
    "proba = tta_result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "\n",
    "tta_result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "tta_result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "tta_result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = tta_result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission_llama_tta.csv', index=False)\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aca6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:25:48.085451Z",
     "iopub.status.busy": "2024-08-02T18:25:48.085173Z",
     "iopub.status.idle": "2024-08-02T18:25:48.104096Z",
     "shell.execute_reply": "2024-08-02T18:25:48.103249Z"
    },
    "papermill": {
     "duration": 0.036575,
     "end_time": "2024-08-02T18:25:48.106341",
     "exception": false,
     "start_time": "2024-08-02T18:25:48.069766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile gemma2b_inference.py\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import  GemmaTokenizerFast, BitsAndBytesConfig, Gemma2ForCausalLM\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel\n",
    "\n",
    "assert torch.cuda.device_count() == 2\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-2b-it/1'\n",
    "    lora_dir = '/kaggle/input/exp58-gemma2b-9143/checkpoint-5400'\n",
    "    max_length = 2200\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = True  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Load & pre-process Data \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "if len(test)<= 10:\n",
    "    test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv', nrows=100)\n",
    "\n",
    "    \n",
    "if cfg.tta:\n",
    "    test['response_a'], test['response_b'] = test['response_b'], test['response_a']\n",
    "    \n",
    "def process(input_str):\n",
    "    return json.loads(input_str)\n",
    "original_length = len(test)\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "test = test.explode(['prompt','response_a','response_b']).reset_index(drop=True)\n",
    "test = test.fillna('None')\n",
    "test['response_a'] = test['response_a'].apply(lambda x: 'None' if len(x) == 0 else x)\n",
    "test['response_b'] = test['response_b'].apply(lambda x: 'None' if len(x) == 0 else x)\n",
    "\n",
    "def get_text_length(text):\n",
    "    '''\n",
    "    不用空格分隔的文本, text length = len\n",
    "    不用空格分隔的一般tokenizer后长度类似，所以还可以缩小\n",
    "    空格分隔的，len(text.split(\" \"))\n",
    "    '''\n",
    "    length1 = len(text)\n",
    "    length2 = len(text.split(\" \"))\n",
    "    #远超过\n",
    "    if length1 >= length2 * 30 and length1>= 300:\n",
    "        return length1 * 0.75\n",
    "    return length2\n",
    "    \n",
    "def prompt_3(data, max_length, if_train):\n",
    "    '''\n",
    "    超过max length新开一行，label不变\n",
    "    从后往前拼接\n",
    "    #Prompt1\n",
    "    xxxx\n",
    "    #Response\n",
    "    ##Model A\n",
    "    xxxx\n",
    "    ##Model B\n",
    "    xxxx\n",
    "    \n",
    "    #Prompt2\n",
    "    #Response\n",
    "    ##Model A\n",
    "    xxxx\n",
    "    ##Model B\n",
    "    xxxx\n",
    "    '''\n",
    "\n",
    "    data['prompt_response'] = \"#Prompt\\n\" + data['prompt'] + \"\\n\\n\" + \"#Response\\n\" + \"##Model A\\n\" + data['response_a'] + \"\\n\\n\" + \"##Model B\\n\" + data['response_b']\n",
    "    data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    prompt_response = []\n",
    "    ids = []\n",
    "    labels = []\n",
    "    #只有一种可能会超出max length：\n",
    "    #单条的prompt和reponse加在一起超出max length\n",
    "    over_max_length = [] #是否有超出max length的部分\n",
    "    overflow_prompt = []\n",
    "    overflow_response_a = [] #超出max length的部分\n",
    "    overflow_response_b = [] #超出max length的部分\n",
    "    text_length = 0\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        text = row['prompt_response']\n",
    "        response_a = row['response_a']\n",
    "        response_b = row['response_b']\n",
    "        prompt = row['prompt']\n",
    "        id = row['id']\n",
    "        \n",
    "        if if_train:\n",
    "            label = row['label']\n",
    "        \n",
    "        if id not in ids:\n",
    "            #第一次出现\n",
    "            prompt_response.append(text)\n",
    "            text_length = get_text_length(text)\n",
    "            ids.append(id)\n",
    "            if if_train:\n",
    "                labels.append(label)\n",
    "            if text_length > max_length:\n",
    "                over_max_length.append(1)\n",
    "                overflow_prompt.append(prompt)\n",
    "                overflow_response_a.append(response_a)\n",
    "                overflow_response_b.append(response_b)\n",
    "            else:\n",
    "                over_max_length.append(0)\n",
    "                overflow_prompt.append(None)\n",
    "                overflow_response_a.append(None)\n",
    "                overflow_response_b.append(None)\n",
    "        \n",
    "        else:\n",
    "            text_length += get_text_length(text)\n",
    "            if text_length <= max_length:\n",
    "                #取上一个text出来，合并后替换\n",
    "                text = text + \"\\n\\n\" + prompt_response[-1]\n",
    "                prompt_response[-1] = text\n",
    "                over_max_length[-1] = 0\n",
    "                overflow_prompt[-1] = None\n",
    "                overflow_response_a[-1] = None\n",
    "                overflow_response_b[-1] = None\n",
    "                \n",
    "            else:\n",
    "                #另一起一行\n",
    "                prompt_response.append(text)\n",
    "                text_length = get_text_length(text)\n",
    "                ids.append(id)\n",
    "                \n",
    "                if if_train:\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                #另起一行但超出场合都\n",
    "                if text_length > max_length:\n",
    "                    over_max_length.append(1)\n",
    "                    overflow_prompt.append(prompt)\n",
    "                    overflow_response_a.append(response_a)\n",
    "                    overflow_response_b.append(response_b)\n",
    "                else:\n",
    "                    over_max_length.append(0)\n",
    "                    overflow_prompt.append(None)\n",
    "                    overflow_response_a.append(None)\n",
    "                    overflow_response_b.append(None)\n",
    "                    \n",
    "                \n",
    "                    \n",
    "    if if_train:           \n",
    "        data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, \"label\": labels, 'overflow_prompt': overflow_prompt, 'over_max_length': over_max_length, 'overflow_response_a': overflow_response_a, 'overflow_response_b': overflow_response_b})\n",
    "        data = data.iloc[::-1].reset_index(drop=True)#反转\n",
    "    else:\n",
    "        data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, 'over_max_length': over_max_length, 'overflow_prompt': overflow_prompt, 'overflow_response_a': overflow_response_a, 'overflow_response_b': overflow_response_b})\n",
    "        data = data.iloc[::-1].reset_index(drop=True)#反转\n",
    "    return data\n",
    "\n",
    "test = prompt_3(test, cfg.max_length * 0.75, False)\n",
    "test = test.drop_duplicates(subset=['id'], keep='last').reset_index(drop=True)\n",
    "assert len(test) == original_length\n",
    "\n",
    "# tokenize\n",
    "\n",
    "def tokenize(\n",
    "    tokenizer, data\n",
    "):\n",
    "    prompts = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        now_data = data.loc[i]\n",
    "        idx = now_data['id']\n",
    "        \n",
    "        over_max_length = now_data['over_max_length']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "\n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    " \n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "\n",
    "        templete_part4_input_ids = tokenizer(text=\"\\n\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "\n",
    "        if over_max_length:\n",
    "            prompt = \"#Prompt\\n\" + now_data['overflow_prompt']\n",
    "            r_a = \"#Response\\n\" + \"##Model A\\n\" + now_data['overflow_response_a']\n",
    "            r_b = \"##Model B\\n\" + now_data['overflow_response_b']\n",
    "\n",
    "            prompt_ids = tokenizer(text=prompt, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_a_input_ids = tokenizer(text=r_a, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_b_input_ids = tokenizer(text=r_b, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "\n",
    "            if len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids) <= cfg.max_length:\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "\n",
    "            else:\n",
    "                '''\n",
    "                prompt 和 response 按照 300， 800， 800\n",
    "                response 优先\n",
    "                多的再给prompt\n",
    "                '''\n",
    "                length = [len(prompt_ids), len(model_a_input_ids), len(model_b_input_ids)]\n",
    "                print(f\"before {len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids)}\")\n",
    "                print(f\"before {length}\")\n",
    "                prompt_max_length, a_max_length, b_max_length = adjust(length)\n",
    "\n",
    "                prompt_ids = prompt_ids[:prompt_max_length] + templete_part4_input_ids\n",
    "                model_a_input_ids = model_a_input_ids[:a_max_length] + templete_part4_input_ids\n",
    "                model_b_input_ids = model_b_input_ids[:b_max_length] + templete_part4_input_ids\n",
    "\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "                print(f\"after {[prompt_max_length, a_max_length, b_max_length]}\")\n",
    "                print(f\"after {len(prompt_response_ids)}\")\n",
    "\n",
    "        else:\n",
    "            prompt_response = now_data['prompt_response']\n",
    "            prompt_response_ids = tokenizer(text=prompt_response, add_special_tokens=True, truncation=True, max_length=cfg.max_length, padding=False)['input_ids'][1:]    \n",
    "\n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = tokenizer.decode(input_ids[1:], skip_special_tokens=False)\n",
    "        if i == 0:\n",
    "            print(input_text)\n",
    "        prompts.append(input_text)\n",
    "    tokenized = tokenizer(prompts)\n",
    "    input_ids = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask \n",
    "\n",
    "def adjust_values(A, B, a_space, b_space, ex_space):\n",
    "    # 计算A和a_space的差值\n",
    "    a_diff = a_space - A\n",
    "    b_diff = b_space - B\n",
    "    \n",
    "    # 第一种情况：A小于a_space，B小于b_space\n",
    "    if A < a_space and B < b_space:\n",
    "        ex_space += a_diff + b_diff\n",
    "        return A, B, ex_space\n",
    "\n",
    "    # 第二种情况：如果A和B都各自大于自己的space\n",
    "    elif A > a_space and B > b_space:\n",
    "        total_extra_needed = (A - a_space) + (B - b_space)\n",
    "        if total_extra_needed > ex_space:\n",
    "            A = int(a_space + ex_space / 2)\n",
    "            B = int(b_space + ex_space / 2)\n",
    "            ex_space = 0\n",
    "        else:\n",
    "            a_space = A\n",
    "            b_space = B\n",
    "            ex_space -= total_extra_needed\n",
    "            \n",
    "        return A, B, ex_space\n",
    "        \n",
    "    # 第三种情况：A或者B其中有一个大于a_space, b_space\n",
    "    elif A >= a_space or B >= b_space:\n",
    "        # 如果A大于a_space但是B小于b_space\n",
    "        if A >= a_space and B <= b_space:\n",
    "            extra_needed = A - a_space\n",
    "            ex_space += b_space - B\n",
    "            #够用\n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                #不够用\n",
    "                #b_space = B + available_space\n",
    "                A = a_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        # 如果B大于b_space但是A小于a_space\n",
    "        elif B > b_space and A < a_space:\n",
    "            extra_needed = B - b_space\n",
    "            ex_space += a_space - A\n",
    "            \n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                B = b_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        return A, B, ex_space\n",
    "    \n",
    "\n",
    "def adjust(current_lengths, prompt_length_space=300, response_length_space=950):\n",
    "    prompt_length = current_lengths[0]\n",
    "    response_a_length = current_lengths[1]\n",
    "    response_b_length = current_lengths[2]\n",
    "    #先看prompt的额度\n",
    "    ex_space = max(0, prompt_length_space - prompt_length)\n",
    "    response_a_length, response_b_length, ex_space = adjust_values(response_a_length, response_b_length, response_length_space, response_length_space, ex_space)\n",
    "    prompt_length = min(prompt_length, prompt_length_space)\n",
    "    prompt_length += ex_space\n",
    "\n",
    "    return prompt_length, response_a_length, response_b_length\n",
    "\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "# tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test)\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))\n",
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "class CustomGemma2ForCausalLM(Gemma2ForCausalLM):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "        if self.training and self.config._attn_implementation != \"eager\":\n",
    "            logger.warning_once(\n",
    "                \"It is strongly recommended to train Gemma2 models with the `eager` attention implementation \"\n",
    "                f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n",
    "            )\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0][:,-1]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if self.config.final_logit_softcapping is not None:\n",
    "            logits = logits / self.config.final_logit_softcapping\n",
    "            logits = torch.tanh(logits)\n",
    "            logits = logits * self.config.final_logit_softcapping\n",
    "\n",
    "        logits = logits.float()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "# Load base model on GPU 0\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = CustomGemma2ForCausalLM.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = CustomGemma2ForCausalLM.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n",
    "\n",
    "model_0.eval()\n",
    "model_1.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference_test(df, model, device, batch_size=cfg.batch_size):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        outputs = model(**inputs.to(device))\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = ((outputs.logits.cpu()[:,A_TOKEN_IDS + B_TOKEN_IDS + C_TOKEN_IDS]) / 1.03).softmax(-1)\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "\n",
    "data = data[:int(len(data) * 0.5)]\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "\n",
    "if cfg.tta:\n",
    "    result_df.loc[:, \"winner_model_a\"] = proba[:, 1]\n",
    "    result_df.loc[:, \"winner_model_b\"] = proba[:, 0]\n",
    "    result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "else:\n",
    "    result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "    result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "    result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission_gemma2b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c71f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:25:48.135910Z",
     "iopub.status.busy": "2024-08-02T18:25:48.135287Z",
     "iopub.status.idle": "2024-08-02T18:28:54.014386Z",
     "shell.execute_reply": "2024-08-02T18:28:54.013438Z"
    },
    "papermill": {
     "duration": 185.896296,
     "end_time": "2024-08-02T18:28:54.016741",
     "exception": false,
     "start_time": "2024-08-02T18:25:48.120445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python llama_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb7d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:28:54.052835Z",
     "iopub.status.busy": "2024-08-02T18:28:54.052540Z",
     "iopub.status.idle": "2024-08-02T18:33:58.120237Z",
     "shell.execute_reply": "2024-08-02T18:33:58.118822Z"
    },
    "papermill": {
     "duration": 304.090037,
     "end_time": "2024-08-02T18:33:58.124092",
     "exception": false,
     "start_time": "2024-08-02T18:28:54.034055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python gemma_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0249b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:33:58.166342Z",
     "iopub.status.busy": "2024-08-02T18:33:58.165952Z",
     "iopub.status.idle": "2024-08-02T18:35:08.628892Z",
     "shell.execute_reply": "2024-08-02T18:35:08.627030Z"
    },
    "papermill": {
     "duration": 70.486777,
     "end_time": "2024-08-02T18:35:08.631406",
     "exception": false,
     "start_time": "2024-08-02T18:33:58.144629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python gemma2b_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da9b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:08.679620Z",
     "iopub.status.busy": "2024-08-02T18:35:08.678536Z",
     "iopub.status.idle": "2024-08-02T18:35:08.685448Z",
     "shell.execute_reply": "2024-08-02T18:35:08.684312Z"
    },
    "papermill": {
     "duration": 0.032782,
     "end_time": "2024-08-02T18:35:08.687552",
     "exception": false,
     "start_time": "2024-08-02T18:35:08.654770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfdf69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:08.733094Z",
     "iopub.status.busy": "2024-08-02T18:35:08.732038Z",
     "iopub.status.idle": "2024-08-02T18:35:09.134533Z",
     "shell.execute_reply": "2024-08-02T18:35:09.133536Z"
    },
    "papermill": {
     "duration": 0.427921,
     "end_time": "2024-08-02T18:35:09.136950",
     "exception": false,
     "start_time": "2024-08-02T18:35:08.709029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "llama_result = pd.read_csv('/kaggle/working/submission_llama_tta.csv')\n",
    "gemma_result = pd.read_csv('/kaggle/working/submission_gemma.csv')\n",
    "gemma2b_result = pd.read_csv('/kaggle/working/submission_gemma2b.csv')\n",
    "\n",
    "llama_result['winner_model_a'], llama_result['winner_model_b'] = llama_result['winner_model_b'], llama_result['winner_model_a']\n",
    "tta_ids = list(gemma2b_result.id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d91b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:09.181244Z",
     "iopub.status.busy": "2024-08-02T18:35:09.180750Z",
     "iopub.status.idle": "2024-08-02T18:35:09.205825Z",
     "shell.execute_reply": "2024-08-02T18:35:09.205032Z"
    },
    "papermill": {
     "duration": 0.049036,
     "end_time": "2024-08-02T18:35:09.207876",
     "exception": false,
     "start_time": "2024-08-02T18:35:09.158840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_result[llama_result.id.isin(tta_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474f36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:09.252372Z",
     "iopub.status.busy": "2024-08-02T18:35:09.252094Z",
     "iopub.status.idle": "2024-08-02T18:35:09.268047Z",
     "shell.execute_reply": "2024-08-02T18:35:09.267305Z"
    },
    "papermill": {
     "duration": 0.040067,
     "end_time": "2024-08-02T18:35:09.269961",
     "exception": false,
     "start_time": "2024-08-02T18:35:09.229894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma2b_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbaaf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:09.315368Z",
     "iopub.status.busy": "2024-08-02T18:35:09.315077Z",
     "iopub.status.idle": "2024-08-02T18:35:09.326398Z",
     "shell.execute_reply": "2024-08-02T18:35:09.325515Z"
    },
    "papermill": {
     "duration": 0.036172,
     "end_time": "2024-08-02T18:35:09.328192",
     "exception": false,
     "start_time": "2024-08-02T18:35:09.292020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4932d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:09.373946Z",
     "iopub.status.busy": "2024-08-02T18:35:09.373673Z",
     "iopub.status.idle": "2024-08-02T18:35:09.386187Z",
     "shell.execute_reply": "2024-08-02T18:35:09.385371Z"
    },
    "papermill": {
     "duration": 0.037499,
     "end_time": "2024-08-02T18:35:09.388130",
     "exception": false,
     "start_time": "2024-08-02T18:35:09.350631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma2b = gemma2b_result[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "llama_a = llama_result[llama_result.id.isin(tta_ids)].reset_index(drop=True)[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "llama_b = llama_result[~llama_result.id.isin(tta_ids)].reset_index(drop=True)[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "\n",
    "gemma_a = gemma_result[gemma_result.id.isin(tta_ids)].reset_index(drop=True)[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "gemma_b = gemma_result[~gemma_result.id.isin(tta_ids)].reset_index(drop=True)[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b960fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:09.434876Z",
     "iopub.status.busy": "2024-08-02T18:35:09.434225Z",
     "iopub.status.idle": "2024-08-02T18:35:09.439550Z",
     "shell.execute_reply": "2024-08-02T18:35:09.438681Z"
    },
    "papermill": {
     "duration": 0.030872,
     "end_time": "2024-08-02T18:35:09.441387",
     "exception": false,
     "start_time": "2024-08-02T18:35:09.410515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(gemma_a.shape, llama_a.shape, gemma2b.shape)\n",
    "proba_a = gemma_a * 0.55 + llama_a * 0.4 + gemma2b * 0.05\n",
    "proba_b = gemma_b * 0.55 + llama_b * 0.45\n",
    "print(proba_a.shape, proba_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e0c9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T18:35:09.487886Z",
     "iopub.status.busy": "2024-08-02T18:35:09.487637Z",
     "iopub.status.idle": "2024-08-02T18:35:09.508057Z",
     "shell.execute_reply": "2024-08-02T18:35:09.507179Z"
    },
    "papermill": {
     "duration": 0.045802,
     "end_time": "2024-08-02T18:35:09.509988",
     "exception": false,
     "start_time": "2024-08-02T18:35:09.464186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import copy\n",
    "final_a = copy.deepcopy(gemma2b_result)\n",
    "final_b = copy.deepcopy(gemma_result[~gemma_result.id.isin(tta_ids)])\n",
    "\n",
    "final_a.loc[:, \"winner_model_a\"] = proba_a[:, 0]\n",
    "final_a.loc[:, \"winner_model_b\"] = proba_a[:, 1]\n",
    "final_a.loc[:, \"winner_tie\"] = proba_a[:, 2]\n",
    "\n",
    "\n",
    "final_b.loc[:, \"winner_model_a\"] = proba_b[:, 0]\n",
    "final_b.loc[:, \"winner_model_b\"] = proba_b[:, 1]\n",
    "final_b.loc[:, \"winner_tie\"] = proba_b[:, 2]\n",
    "\n",
    "submission_df = pd.concat([final_a, final_b]).sort_values(\"id\").reset_index(drop=True)[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5308079,
     "sourceId": 8823073,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5152072,
     "sourceId": 8823248,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5333496,
     "sourceId": 8859134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5381707,
     "sourceId": 8943761,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5385303,
     "sourceId": 8949013,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5390617,
     "sourceId": 8956886,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5408178,
     "sourceId": 8981123,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5416061,
     "sourceId": 8992006,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5420918,
     "sourceId": 8999200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5430800,
     "sourceId": 9013400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5436768,
     "sourceId": 9021926,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5437896,
     "sourceId": 9023427,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5455572,
     "sourceId": 9048567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5459673,
     "sourceId": 9054559,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5460081,
     "sourceId": 9055332,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5471342,
     "sourceId": 9070908,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 9084522,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5485753,
     "sourceId": 9090829,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 66780,
     "sourceId": 79488,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72254,
     "sourceId": 85995,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 614.144662,
   "end_time": "2024-08-02T18:35:09.965315",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-02T18:24:55.820653",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f59addf0d2f40309e025976c382cad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_268e3946106b4e41849bf11c5a375dac",
       "placeholder": "​",
       "style": "IPY_MODEL_5bb130c471af4927a6644f932ae47523",
       "value": " 2/2 [00:03&lt;00:00,  1.48s/it]"
      }
     },
     "19ef2d43bafa44a8b20dc5230aea5ae4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ca042ffa14e4dbebdc66435f7b1f07f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4",
       "placeholder": "​",
       "style": "IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d03719518b8423099b8b68a92e449d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64ef70cfa9c04764868fa52963323322",
       "placeholder": "​",
       "style": "IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d89705c74d34016bbc1e0601ead825c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d409334237014614bf9ae742597d98ea",
       "placeholder": "​",
       "style": "IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6",
       "value": " 2/2 [01:18&lt;00:00, 34.68s/it]"
      }
     },
     "1ef6d64d40d8461d9e6adddd513089b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "268e3946106b4e41849bf11c5a375dac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "324c2396f44f45c89d9ec264007ef9ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5bb130c471af4927a6644f932ae47523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5d243712a1a545e99fa858b0cf19831d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e81b324ca1b46a2a96d02bb2acadc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64ef70cfa9c04764868fa52963323322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ae4b02a13f4570ad729c437ebd28ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f",
        "IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef",
        "IPY_MODEL_0f59addf0d2f40309e025976c382cad8"
       ],
       "layout": "IPY_MODEL_9422a87b93ab4473913e601da3a18689"
      }
     },
     "81b4a9a7cde64b17856b89dbd238c0ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff",
       "value": 2
      }
     },
     "85d217d6b45847869cea506db59e8b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d243712a1a545e99fa858b0cf19831d",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96385fd98f304649ab5c4ae81333fb63",
       "value": 2
      }
     },
     "875758123e4f41f0b5c3fa0cd4fb47c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9422a87b93ab4473913e601da3a18689": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96385fd98f304649ab5c4ae81333fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d409334237014614bf9ae742597d98ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d576a283e6424206ab4c25d809241c21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8a7714cd80d479e859b6ae31ebce7e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d98918fae8174629b4819a1114f21202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d03719518b8423099b8b68a92e449d7",
        "IPY_MODEL_85d217d6b45847869cea506db59e8b42",
        "IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"
       ],
       "layout": "IPY_MODEL_d576a283e6424206ab4c25d809241c21"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
